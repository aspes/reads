Chisel natively supports asynchronous reset registers since version 3.2.0 by allowing designers to specify an AsyncReset() type for the reset signal, 
which the FIRRTL compiler then infers to produce asynchronous Verilog flip-flops. 

In contrast, a Bool() type for a reset signal will result in a synchronously reset flip-flop.
 
How to use asynchronous resets in Chisel 
Declare an AsyncReset type for your reset signal :
Code

    val myAsyncReset = IO(Input(AsyncReset()))
Instantiate a register with this asynchronous reset : 
Code

    // Create a register with a synchronous reset
    val syncReg = Reg(init = 0.U(8.W))

    // Create a register with an asynchronous reset
    val asyncReg = withClockAndReset(clk, myAsyncReset) {
      Reg(init = 0.U(8.W))
    }
Chisel generates the appropriate Verilog : 
For syncReg (using a Bool type):
Code

        always @(posedge clk) begin
          if (reset) begin
            syncReg <= 1'h0;
          end else begin
            syncReg <= io_in;
          end
        end
For asyncReg (using AsyncReset). 
Code

        always @(posedge clk or posedge myAsyncReset) begin
          if (myAsyncReset) begin
            asyncReg <= 1'h0;
          end else begin
            asyncReg <= io_in;
          end
        end
Key differences 
Bool():
Creates a synchronous reset, which takes effect only on the active clock edge.
AsyncReset():
Creates an asynchronous reset, which takes effect immediately upon assertion, independently of the clock edge.

---

# Chisel’s support for asynchronous reset register
As of Chisel version 3.2.0, Chisel natively supports asynchronous reset registers through the AsyncReset data type. 
The generated Verilog output for a register depends on the type of its associated reset signal. 
If the reset is of type AsyncReset, Chisel will produce an always block that includes the reset signal in its sensitivity list, 
allowing it to assert immediately, independent of the clock edge. 

Defining an asynchronous reset register
The key to creating an asynchronous reset register in Chisel is to define the reset signal itself as AsyncReset. 
You can then use this signal within a withClockAndReset scope to associate it with a new register. 

Example code
```scala
import chisel3._
import chisel3.stage.ChiselStage

class AsyncResetModule extends RawModule {
  val clk = IO(Input(Clock()))
  val reset = IO(Input(AsyncReset())) // Define the reset as AsyncReset
  val io = IO(new Bundle {
    val D = Input(Bool())
    val out = Output(Bool())
  })

  // Use the custom clock and reset for this scope
  withClockAndReset(clk, reset) {
    // This register will have an asynchronous reset
    val x = RegNext(io.D, init = false.B)
    io.out := x
  }
}
```
Use code with caution.

Generated Verilog

The Chisel code above will generate Verilog that reflects the asynchronous reset behavior: 
```verilog
always @(posedge clk or posedge reset) begin
  if (reset) begin
    x <= 1'h0;
  end else begin
    x <= io_D;
  end
end
```
Use code with caution.

Reset data types in Chisel

Chisel differentiates between three types of resets, which dictate the behavior of the generated hardware: 
- AsyncReset: This type constructs an asynchronously reset register, where the reset signal is sensitive to its own assertion, not the clock edge.
- Bool: This standard Bool type is used for synchronous resets. A register with a Bool reset will only apply the reset on the active clock edge.
- Reset: This is an abstract type that allows the reset behavior to be inferred during FIRRTL (the intermediate representation of Chisel) compilation. It can resolve to either a synchronous (Bool) or asynchronous (AsyncReset) reset depending on the context. 

Key considerations for asynchronous resets
- Synchronization: The asynchronous nature of AsyncReset can lead to metastability issues when the reset is de-asserted. The reset signal must be synchronized to the clock domain before its de-assertion for proper circuit operation.
- Modularity: To use an asynchronous reset, you must declare your module as RawModule or explicitly provide the withClockAndReset scope. Standard Module instances have an implicit synchronous reset.
- Design tradeoffs: While asynchronous resets can provide faster and more immediate reset functionality, 
they can also complicate timing analysis and require careful design practices to avoid issues. 

---

Designing Spiking Neural Network-Based Reinforcement Learning for 3D Robotic Arm Applications

# SNN-based TD3 algorithm
An SNN-based TD3 algorithm replaces the standard artificial neural networks (ANNs) 
in the Twin Delayed Deep Deterministic Policy Gradient (TD3) framework with Spiking Neural Networks (SNNs) 
to achieve greater energy efficiency, especially for neuromorphic hardware. 

This integration is challenging due to the discrete, binary nature of SNN "spikes," which differs from the continuous, differentiable outputs of ANNs. 

## Core components and modifications
The SNN-based TD3 algorithm adapts the standard TD3 framework to work with SNNs. 

### TD3 components

Twin Critics: Uses two critic networks to minimize overestimation bias.

Delayed updates: Updates the policy (actor) less frequently than the Q-functions (critics).

Target policy smoothing: Adds clipped noise to the target action to make the policy more robust. 

### SNN modifications
SNN Actor: The actor network, which produces a continuous action output, is replaced with an SNN.

ANN Critics: The two critic networks, which estimate continuous Q-values, are typically kept as ANNs. 
This hybrid approach is necessary because SNNs are not well-suited for the continuous Q-value estimation required by the critic.

Spike-encoding: The continuous state-space observation from the environment must be encoded into a series of spike trains that the SNN can process. 
Common methods include rate coding, where the firing rate represents the input value, or population coding, 
where different groups of neurons represent different input values.

Surrogate gradients: The discontinuous nature of SNNs makes traditional backpropagation training difficult. 
To solve this, surrogate gradient methods are used. These methods provide an approximation of the gradient, allowing the network to be trained with backpropagation.

Bridging framework: A 2025 paper proposes a "proxy target" framework to stabilize training. It replaces the SNN target actor with a continuous, 
differentiable proxy network that is only used during training. 
This prevents instability caused by discrete SNN spikes while allowing for low-power SNN deployment. 

### Training and deployment
The training process must account for the specific characteristics of the SNN.

Exploration: To handle exploration with SNNs, methods like "Noisy Spiking Actor Network (NoisySAN)" 
have been developed to introduce time-correlated noise during the spike-generation process, facilitating exploration.

Training loop: During the training phase, the ANN critics and the SNN actor (with potential proxy network assistance) 
are updated using backpropagation with surrogate gradients. 
The critics learn to estimate the Q-values, while the actor learns to maximize these Q-values.

Deployment: For inference or deployment on a neuromorphic chip, the trained SNN actor can be used directly without the ANN critics or proxy networks, 
preserving the energy efficiency benefits. 

### Benefits and applications
This approach combines the advantages of TD3's stable training and SNNs' energy efficiency, making it suitable for edge-computing and robotics. 

Benefits

Energy efficiency: SNNs are highly energy-efficient compared to ANNs, which is crucial for resource-constrained applications.

Real-time performance: SNNs have low-latency capabilities and can process information effectively in real-time.

Neuromorphic hardware friendly: SNNs are designed to run on specialized neuromorphic hardware, which can accelerate computation while reducing power consumption. 

Applications

Robotic arm control: An SNN-based TD3 has been used for 3D robotic arm manipulation, 
demonstrating better energy efficiency and stability compared to traditional methods.

Autonomous navigation: This algorithm is applicable to other robotic systems, 
such as unmanned surface vehicles, to optimize navigation and obstacle avoidance.

Energy-efficient locomotion: Studies have applied SNNs to deep reinforcement learning (DRL) algorithms to find energy-efficient gaits for hexapod robots.
 
---

[Innatera’s chip promises lower latency and power consumption for edge AI](https://spectrum.ieee.org/innatera-neuromorphic-chip#:~:text=The%20Pulsar%20chip%20possesses%20a,2.6%20millimeters%2C%E2%80%9D%20Kumar%20says.)

---

There is no single "best" Spiking Neural Network (SNN) model; the optimal choice depends on the specific application, 
computational resources, and desired level of biological plausibility. For frameworks, consider PyTorch-based options 
like snnTorch or SpikingJelly, which support gradient-based training. 

For neuron models, Leaky Integrate-and-Fire (LIF) is efficient but less realistic, 
while Izhikevich and Spike-Response Model (SRM) offer greater biological detail but are more complex. 

Hybrid architectures, like Spikeformer, and training methods such as surrogate gradient descent, are also vital components of high-performance SNNs. 

## Factors to Consider When Choosing an SNN Model
Application:
What task are you trying to solve? Some models excel at vision, others at language, and some at dynamic, temporal tasks. 

Computational Resources:
SNNs can run on standard hardware or dedicated neuromorphic hardware, which benefits from their event-driven nature. 

Biological Plausibility vs. Efficiency:
More complex neuron models like Izhikevich or SRM are more realistic but can be computationally expensive, 
whereas simple Integrate-and-Fire (IF) or Leaky Integrate-and-Fire (LIF) models are highly efficient. 

Gradient-Based Training:
For deep networks, frameworks that support gradient-based training using surrogate gradients (like snnTorch and SpikingJelly) are often preferred. 

### Examples of SNN Models and Frameworks
Neuron Models:

Leaky Integrate-and-Fire (LIF): A common and efficient model that integrates inputs and fires when a threshold is reached, 
with a leakage mechanism to prevent infinite accumulation. 

Izhikevich Model: A model that offers greater biological realism by capturing the dynamics of action potential generation and recovery. 

Spike-Response Model (SRM): A generalized model that incorporates features like refractory periods, providing a more detailed description of neuron behavior. 

Architectures and Frameworks:
Transformer-based SNNs (e.g., Spikeformer): These models leverage the attention mechanisms of transformers to process spatio-temporal data, 
often achieving a good balance between accuracy and complexity. 

Frameworks like snnTorch and SpikingJelly: These PyTorch-based frameworks provide robust tools for building and training SNNs, 
enabling GPU acceleration and gradient-based learning. 

Forward-Forward Algorithm: Emerging backpropagation-free approaches, like those using the Forward-Forward algorithm adapted for SNNs, 
offer energy-efficient and biologically plausible alternatives for training. 

Hybrid Models: Combining established deep learning components (like ResNets or Swin Transformers) with SNN elements 
can harness the strengths of both approaches for complex tasks. 

---

The Forward-Forward (FF) algorithm, originally proposed for artificial neural networks (ANNs), has been adapted for use with Spiking Neural Networks (SNNs) as an alternative to backpropagation. This adaptation leverages the FF algorithm's core principle of using two forward passes—one with "positive" (real) data and another with "negative" (contrasting) data—to train each layer independently.

How FF is Adapted for SNNs:

Positive and Negative Data Generation:
Positive Samples: Real labeled data, potentially with the correct label embedded into the input.
Negative Samples: Generated by corrupting positive samples or using synthetic data, often by embedding an incorrect label into the input to create a structural difference from positive data.

Layer-wise Objective Function:
Each layer in the SNN is trained to maximize a "goodness" function for positive data and minimize it for negative data. For SNNs, this "goodness" can be related to the sum of squared activities (spike rates or membrane potentials) within a layer, or other SNN-compatible metrics.

Two Forward Passes:

Positive Pass: The SNN processes the positive data, aiming to achieve high "goodness" values in its layers.

Negative Pass: The SNN processes the negative data, aiming to achieve low "goodness" values in its layers.

Local Learning:
Unlike backpropagation, which requires gradients to be propagated backward through the entire network, the FF algorithm allows for localized learning within each layer based on its own objective function and the two forward passes.

Advantages for SNNs:

Backpropagation-free:
Eliminates the need for complex gradient calculations and backpropagation through time, which can be challenging for SNNs due to their non-differentiable spiking activation functions.

Computational Efficiency:
The localized learning and two forward passes can potentially lead to more efficient training, especially on neuromorphic hardware.

Hardware Compatibility:
The simplicity and locality of the FF algorithm make it well-suited for implementation on neuromorphic hardware designed for SNNs.

Biological Plausibility:
The layer-wise learning and absence of a global error signal are often considered more biologically plausible than backpropagation.

Challenges:
Negative Data Generation:
Generating high-quality negative data that effectively challenges the network remains a critical aspect for achieving optimal performance.

Accuracy:
While promising, some FF-based SNN methods may still face challenges in achieving the same level of accuracy as backpropagation-trained SNNs or ANNs in certain complex tasks.

---

Spiking neural network github

Key GitHub repositories for spiking neural networks (SNNs) include popular libraries like snnTorch, SpikingJelly, and BindsNET. 

The choice of library depends on your experience level and specific research or application goals. 

## SNN libraries and frameworks

snnTorch is a Python package built on PyTorch for deep and online learning with SNNs.

Key features: Seamless integration with PyTorch, tutorials, and a supportive Discord community.

GitHub: jeshraghian/snntorch

SpikingJelly is another open-source SNN framework also based on PyTorch.

Key features: Known for being user-friendly, with tutorials that make SNN creation as simple as building standard ANNs in PyTorch.

GitHub: fangwei123456/spikingjelly

BindsNET is an SNN simulation library using PyTorch Tensor functionality, developed by researchers at Tufts University.

Key features: Geared toward biologically inspired machine learning and reinforcement learning, with detailed examples available.

GitHub: BindsNET/bindsnet

Rockpool is a machine learning library for SNNs that works with both PyTorch and JAX pipelines, and can deploy models to neuromorphic hardware.

GitHub: synsense/rockpool

Spike-Element-Wise ResNet is a repository containing code for the paper Deep Residual Learning in Spiking Neural Networks, a notable publication from NeurIPS 2021.

GitHub: fangwei123456/Spike-Element-Wise-ResNet 

Awesome SNN collections
For a broader overview of SNN research and resources, "awesome" lists on GitHub are highly valuable.
coderonion/awesome-snn offers a curated collection of SNN resources, including implementations and research papers.
zhouchenlin2096/Awesome-Spiking-Neural-Networks lists more resources and research papers, with a focus on deep learning with SNNs.

yfguo91/Awesome-Spiking-Neural-Networks is another extensive list of resources, including many projects and papers from recent conferences. 

### Hardware and specialized implementations

ChFrenkel/tinyODIN is a repository for a low-cost, open-source digital SNN processor based on the leaky integrate-and-fire (LIF) neuron model.

genn-team/ml_genn is a library for deep learning with SNNs that is powered by GeNN, a GPU-enhanced neuronal network simulation environment. 

---

GitHub hosts numerous repositories related to Spiking Neural Networks (SNNs), ranging from full-fledged frameworks to specific research implementations and educational resources.
Key SNN-related repositories and projects found on GitHub include:
Frameworks:
SpikingJelly: A PyTorch-based deep learning framework specifically designed for SNNs, offering tools for SNN initialization and ANN-to-SNN conversion.
BindsNET: A PyTorch-based library for simulating SNNs.
BrainPy: A simulation toolbox focused on computational neuroscience research, which includes SNN capabilities.
snn-toolbox: A toolbox for converting Artificial Neural Networks (ANNs) into SNNs using weight normalization techniques.
ml_genn: A library for deep learning with SNNs built on top of GeNN.
Research Implementations:
Spikformer: Official implementation of a research paper exploring the integration of SNNs with Transformer architectures.
ICLR_TINY_SNN: Official implementation of a paper on SNNs incorporating temporal attention and adaptive spiking neurons.
object-detection-with-spiking-neural-networks: Code for a paper on object detection using SNNs on automotive event data.
SpikeNet: Implementation for a paper on scaling up dynamic graph representation learning with SNNs.
Educational Resources:
Spiking-Neural-Networks-Tutorials: A collection of tutorials for training SNNs, drawing lessons from deep learning.
Awesome-Spiking-Neural-Networks: A curated list of papers, documentation, and code related to SNNs.
These repositories provide a diverse range of resources for researchers, developers, and learners interested in Spiking Neural Networks.

---

# How to design spiking neural network chips
From Neurons to Chips: Exploring the Future of Spiking ...
Designing a spiking neural network (SNN) chip, also known as a neuromorphic chip, requires a multidisciplinary approach that mimics the structure and function of the brain to achieve high energy efficiency and parallel processing. The design process involves selecting an architecture, implementing biological neuron and synapse models, integrating a network-on-chip (NoC), and choosing between analog, digital, or mixed-signal circuit techniques. 
1. High-level architecture design
Massive parallelism and distributed computation: Unlike traditional CPUs, SNN chips feature many small, simple processing units (neurons) that compute locally and in parallel. The memory is distributed with the processing, minimizing the power-hungry data movement found in conventional von Neumann architectures.
Network-on-chip (NoC): To manage the large volume of asynchronous spike communication between thousands or millions of neurons, a scalable NoC architecture is essential. The NoC uses an event-based (spike-based) communication protocol, such as Address-Event Representation (AER), which only transmits data when a neuron fires, conserving energy.
Hardware-software co-design: The chip architecture must be designed in tandem with the SNN algorithms and models that will run on it. This includes supporting specific learning rules and network topologies. 
Neuromorphic SoC Design Essentials - Number Analytics
Jun 17, 2025 — Definition and Principles of Neuromorphic Computing Neuromorphic computing is based on the idea of replicating the behavior of biological neurons and synapses u...

Favicon
Number Analytics
TrueNorth: A Deep Dive into IBM's Neuromorphic Chip Design
Mar 27, 2023 — A neuromorphic chip, in principle, is an in-memory computing architecture: in this, there are not a central memory and a central processing unit, but storage an...

Favicon
Open Neuromorphic
2. Neuron and synapse circuit implementation
Neurons
The core of an SNN chip is its neuron model, typically implemented as an electronic circuit.
Leaky Integrate-and-Fire (LIF): This is the most common model due to its low hardware resource requirements and good balance between biological realism and implementation simplicity. The circuit integrates input spikes and "leaks" charge over time until it reaches a threshold, at which point it fires and resets its potential.
More complex models: Other models, such as the Izhikevich neuron, offer richer dynamics but at a higher hardware cost. 
Synapses
Synapses store the network's weights and control the strength of connections between neurons.
Memory technologies: Synaptic weights can be stored using various on-chip memory technologies, including static random-access memory (SRAM), dynamic random-access memory (DRAM), or emerging non-volatile memories (NVMs) like memristors for higher density.
Plasticity circuits: To enable on-chip learning, dedicated circuits implement synaptic plasticity rules like Spike-Timing-Dependent Plasticity (STDP). These circuits modify synaptic weights based on the relative timing of pre- and post-synaptic spikes. 
3. Circuit design approach
The choice between digital, analog, or mixed-signal design is a fundamental trade-off between energy efficiency, precision, and scalability. 
Analog SNN chips
Benefits: Offer extremely high energy efficiency by using continuous-time, continuous-valued circuits that directly emulate biological neural dynamics. They can be very compact.
Drawbacks: Are highly sensitive to manufacturing variations (device mismatch), which can affect the performance and consistency between chips. This often requires complex calibration techniques. 
Digital SNN chips
Benefits: Are highly scalable and programmable, with better precision and resilience to manufacturing variations. They are compatible with standard CMOS fabrication processes.
Drawbacks: Consum higher energy per spike and computation compared to analog designs, as they require digital logic to simulate continuous-time neural behavior. 
Mixed-signal SNN chips
Benefits: Combine the best of both worlds, using efficient analog circuits for critical computations (e.g., neuron integration) and digital circuits for robust communication and control.
Drawbacks: The interface between analog and digital domains adds complexity and can introduce energy overhead through conversions. 
4. Training and learning mechanisms
On-chip learning: For tasks that require real-time adaptation, the chip's learning algorithm is implemented in hardware. STDP is a common choice, but its hardware implementation can be complex.
Supervised learning via proxy: Many state-of-the-art SNN chips are trained using supervised learning, where a pre-trained ANN is converted into an SNN. 
The chip then runs the SNN model in a highly energy-efficient, inference-only mode.

5. Integration with sensors

For many applications, SNN chips are tightly integrated with event-based sensors, such as Dynamic Vision Sensors (DVS).
Sensing-computing coupling: This integration eliminates data redundancy at the source by converting visual information into asynchronous, event-based "spikes" directly at the sensor level. This is far more energy-efficient than capturing full frames.
On-chip pre-processing: The chip often includes pre-processing hardware to format the event stream from the sensor before it is sent to the main SNN cores. 
